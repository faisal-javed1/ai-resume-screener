{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "from tika import parser\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Parsing\n",
    "parsed_content = {}\n",
    "\n",
    "def tparser(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    from spacy.matcher import Matcher\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    def extract_categ(text):\n",
    "        nlp_text = nlp(text)\n",
    "        pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "        matcher.add('NAME', [pattern], on_match=None)\n",
    "        matches = matcher(nlp_text)\n",
    "        for match_id, start, end in matches:\n",
    "            span = nlp_text[start:end]\n",
    "            return span.text\n",
    "\n",
    "    name = extract_categ(text)\n",
    "    parsed_content['Category'] = name\n",
    "    Keywords = [\"education\", \"summary\", \"accomplishments\", \"executive profile\", \"professional profile\",\n",
    "                \"personal profile\", \"work background\", \"academic profile\", \"other activities\", \"qualifications\",\n",
    "                \"experience\", \"interests\", \"skills\", \"achievements\", \"publications\", \"publication\", \"certifications\",\n",
    "                \"workshops\", \"projects\", \"internships\", \"trainings\", \"hobbies\", \"overview\", \"objective\",\n",
    "                \"position of responsibility\", \"jobs\"]\n",
    "\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "    re.sub('\\W+', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    content = {}\n",
    "    indices = []\n",
    "    keys = []\n",
    "    for key in Keywords:\n",
    "        try:\n",
    "            content[key] = text[text.index(key) + len(key):]\n",
    "            indices.append(text.index(key))\n",
    "            keys.append(key)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    zipped_lists = zip(indices, keys)\n",
    "    sorted_pairs = sorted(zipped_lists)\n",
    "    tuples = zip(*sorted_pairs)\n",
    "    indices, keys = [list(tuple) for tuple in tuples]\n",
    "\n",
    "    content = []\n",
    "    for idx in range(len(indices)):\n",
    "        if idx != len(indices) - 1:\n",
    "            content.append(text[indices[idx]: indices[idx + 1]])\n",
    "        else:\n",
    "            content.append(text[indices[idx]:])\n",
    "    for i in range(len(indices)):\n",
    "        parsed_content[keys[i]] = content[i]\n",
    "\n",
    "    return parsed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/data/data/INFORMATION-TECHNOLOGY'\n",
    "data = {'Category': [''], 'Skills': [''], 'Education': [''], 'Experience': ['']}\n",
    "for filename in os.listdir(folder_path):\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        file_data = parser.from_file(file_path)\n",
    "        text = file_data['content']\n",
    "        parsed_content = tparser(text)\n",
    "        data['Category'].append(parsed_content.get('Category', ''))\n",
    "        data['Skills'].append(parsed_content.get('skills', ''))\n",
    "        data['Education'].append(parsed_content.get('education', ''))\n",
    "        data['Experience'].append(parsed_content.get('experience', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Descriptions Extracted!\n"
     ]
    }
   ],
   "source": [
    "#df_res = pd.DataFrame(data)\n",
    "#df_res.to_csv('parsed_res_it.csv')\n",
    "#rint('Resume Parsing Done!')\n",
    "# Extracting Job Descriptions\n",
    "df_res = pd.read_csv('parsed_res_it.csv')\n",
    "raw_df = load_dataset('jacob-hugging-face/job-descriptions', split='train')\n",
    "df_jds = pd.DataFrame(raw_df)\n",
    "df_jds = df_jds.sample(frac=1, random_state=42).head(15)\n",
    "print('Job Descriptions Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Descriptions Cleaned!\n"
     ]
    }
   ],
   "source": [
    "def cleanJD(text):\n",
    "    cleaned_text = text.replace(\"\\n\", \" \")\n",
    "    cleaned_text = cleaned_text.replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]|_', ' ', cleaned_text)\n",
    "    re.sub('\\W+', '', cleaned_text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "skills = []\n",
    "for i in range(0, 15):\n",
    "    text = df_jds['model_response'].str.split(',').iloc[i]\n",
    "    skills.append(cleanJD(text[1]))\n",
    "df_jds['skills'] = skills\n",
    "df_jds = df_jds.drop(columns=['job_description', 'model_response', 'description_length'])\n",
    "df_jds.to_csv('jds.csv', index=False)\n",
    "print('Job Descriptions Cleaned!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Calculation Started!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Similarity Calculation\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "df_res = df_res.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_res = df_res[1:22]\n",
    "df_res.drop(7)\n",
    "sentences_res = df_res[\"Skills\"].to_list()\n",
    "sentences_res = [s for s in sentences_res if isinstance(s, str)][:20]\n",
    "sentences_jds = df_jds[\"skills\"].to_list()\n",
    "sentences_jds = [s for s in sentences_jds if isinstance(s, str)]\n",
    "print('Similarity Calculation Started!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def tokenize_sentence(sentences):\n",
    "    tokens = {'input_ids': [], 'attention_mask': []}\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            new_tokens = tokenizer.encode_plus(sentence, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
    "            tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "            tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence: {sentence}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens_res = tokenize_sentence(sentences_res)\n",
    "tokens_jds = tokenize_sentence(sentences_jds)\n",
    "\n",
    "outputs_res = model(**tokens_res)\n",
    "outputs_jds = model(**tokens_jds)\n",
    "print('Similarity Calculation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_res = outputs_res.last_hidden_state\n",
    "embeddings_jds = outputs_jds.last_hidden_state\n",
    "\n",
    "attention_mask_res = tokens_res['attention_mask']\n",
    "attention_mask_jds = tokens_jds['attention_mask']\n",
    "\n",
    "resized_attention_mask_res = attention_mask_res.unsqueeze(-1).expand(embeddings_res.size()).float()\n",
    "resized_attention_mask_jds = attention_mask_jds.unsqueeze(-1).expand(embeddings_jds.size()).float()\n",
    "\n",
    "masked_embedding_res = embeddings_res * resized_attention_mask_res\n",
    "masked_embedding_jds = embeddings_jds * resized_attention_mask_jds\n",
    "\n",
    "summed_masked_embeddings_res = torch.sum(masked_embedding_res, 1)\n",
    "summed_masked_embeddings_jds = torch.sum(masked_embedding_jds, 1)\n",
    "\n",
    "count_of_one_in_mask_tensor_res = torch.clamp(resized_attention_mask_res.sum(1), min=1e-9)\n",
    "count_of_one_in_mask_tensor_jds = torch.clamp(resized_attention_mask_jds.sum(1), min=1e-9)\n",
    "\n",
    "mean_pooled_res = summed_masked_embeddings_res / count_of_one_in_mask_tensor_res\n",
    "mean_pooled_jds = summed_masked_embeddings_jds / count_of_one_in_mask_tensor_jds\n",
    "\n",
    "mean_pooled_res = mean_pooled_res.detach().numpy()\n",
    "mean_pooled_jds = mean_pooled_jds.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cosine_similarity([mean_pooled_jds[0]], mean_pooled_res[0:])\n",
    "print(f\"Job: {df_jds['position_title'][0]}\")\n",
    "\n",
    "categs = df_res['Category']\n",
    "data = {'Category': categs[1:], 'Scores': res[0]}\n",
    "df_scores = pd.DataFrame(data)\n",
    "df_scores = df_scores.sort_values(by='Scores', ascending=False)\n",
    "print(df_scores[:5])\n",
    "df_scores.to_csv('scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
